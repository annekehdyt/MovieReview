{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stats for Human-Provided Rationales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we experiment with different regular expressions to parse the documents and the human-provided rationales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import csv\n",
    "import operator\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def load_imdb_data(path_to_imdb):\n",
    "    print(\"Loading the imdb reviews data\")\n",
    "    train_neg_files = glob(path_to_imdb + r\"/train/neg/*.txt\")\n",
    "    train_pos_files = glob(path_to_imdb + r\"/train/pos/*.txt\")\n",
    "    train_corpus = []\n",
    "    y_train = []\n",
    "    for tnf in train_neg_files:\n",
    "            with open(tnf, 'r', errors='replace') as f:\n",
    "                line = f.read()\n",
    "                train_corpus.append(line)\n",
    "                y_train.append(0)\n",
    "\n",
    "    for tpf in train_pos_files:\n",
    "        with open(tpf, 'r', errors='replace') as f:\n",
    "            line = f.read()\n",
    "            train_corpus.append(line)\n",
    "            y_train.append(1)\n",
    "\n",
    "    test_neg_files = glob(path_to_imdb + r\"/test/neg/*.txt\")\n",
    "    test_pos_files = glob(path_to_imdb + r\"/test/pos/*.txt\")\n",
    "\n",
    "    test_corpus = []\n",
    "\n",
    "    y_test = []\n",
    "\n",
    "    for tnf in test_neg_files:\n",
    "        with open(tnf, 'r', errors='replace') as f:\n",
    "            test_corpus.append(f.read())\n",
    "            y_test.append(0)\n",
    "\n",
    "    for tpf in test_pos_files:\n",
    "        with open(tpf, 'r', errors='replace') as f:\n",
    "            test_corpus.append(f.read())\n",
    "            y_test.append(1)\n",
    "\n",
    "    print(\"Data loaded.\")\n",
    "    return train_corpus, y_train, test_corpus, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def parse_human_phrases(phrase_file, to_lower=False):\n",
    "    with open(phrase_file) as csvfile:\n",
    "        reader=csv.reader(csvfile, delimiter='\\t')\n",
    "        phrases={} # key:phrase value:number of times appeared in the collection\n",
    "        for row in reader:            \n",
    "            for phrase in row:\n",
    "                if phrase is not \"\":\n",
    "                    if to_lower:\n",
    "                        phrase = phrase.lower()\n",
    "                    if phrase not in phrases:\n",
    "                        phrases[phrase] = 1\n",
    "                    else:\n",
    "                        phrases[phrase] += 1\n",
    "    return phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path_to_imdb = \"C:/Users/mbilgic/Desktop/aclImdb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the imdb reviews data\n",
      "Data loaded.\n"
     ]
    }
   ],
   "source": [
    "train_corpus, y_train, test_corpus, y_test = load_imdb_data(path_to_imdb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_corpus = np.array(train_corpus)\n",
    "test_corpus = np.array(test_corpus)\n",
    "y_train = np.array(y_train)\n",
    "y_test = np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Test for single character\n",
    "# Test for .,?!;\n",
    "# Test for '/<>\"\n",
    "test_case = \"Here is a sentence. I'm here! Are you?? Here, there. No space between.dot. Give it a 3/10. \\\"Quote\\\" Many.<br /><br\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Here is sentence here Are you Here there No space between dot Give it 10 Quote Many br br'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = re.compile(r\"(?u)\\b\\w\\w+\\b\") # the default token pattern\n",
    "\" \".join(tokenizer.findall(test_case))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Here is a sentence I m here Are you Here there No space between dot Give it a 3 10 Quote Many br br'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_pattern = r\"(?u)\\b\\w+\\b\"\n",
    "tokenizer = re.compile(token_pattern)\n",
    "\" \".join(tokenizer.findall(test_case))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Here is a sentence I'm here Are you Here there No space between.dot Give it a 3/10 Quote Many.<br br\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_pattern = r\"(?u)\\b\\S+\\b\"\n",
    "tokenizer = re.compile(token_pattern)\n",
    "\" \".join(tokenizer.findall(test_case))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Here is a sentence I'm here Are you Here there No space between dot Give it a 3/10 Quote Many br br\""
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_pattern = r\"(?u)\\b[\\w\\'/]+\\b\"\n",
    "tokenizer = re.compile(token_pattern)\n",
    "\" \".join(tokenizer.findall(test_case))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "neg_phrases_dict = parse_human_phrases('Negative.tsv', to_lower=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "neg_set = set()\n",
    "for k in neg_phrases_dict.keys():\n",
    "    p = \" \".join(tokenizer.findall(k))\n",
    "    if p != '':\n",
    "        neg_set.add(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "neg_list = list(neg_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(vocabulary=neg_list, lowercase=True, ngram_range=(1,20), binary=True, token_pattern=token_pattern)\n",
    "X = vectorizer.fit_transform(train_corpus[y_train==0])\n",
    "vocab = vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_counts = np.sum(X, axis=0)\n",
    "all_counts_array = all_counts.A1\n",
    "all_counts_sorted_indices = np.argsort(all_counts_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(all_counts_array==0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "his little pile of garbage\n",
      "cenes that are so bad you can have a good laugh at them\n",
      "ails to distinguish himself in the title role\n",
      "his is awkward bad\n",
      "as placed on the video nasties list\n",
      "ne loses hope for it early on\n",
      "adly dubbed poorly acted and slow\n",
      "ast is wasted\n",
      "oo many non native english speakers play parts of native english speakers\n",
      "ever got comfortable with her\n",
      "big emotional moments and climaxes and character relationships come completely out of no where\n",
      "ome gross close ups of the cannibals mouth\n"
     ]
    }
   ],
   "source": [
    "for i in all_counts_sorted_indices:\n",
    "    if all_counts_array[i] > 0:\n",
    "        break\n",
    "    terms = tokenizer.findall(vocab[i])\n",
    "    if len(terms) <= 20:\n",
    "        print(vocab[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pos_phrases_dict = parse_human_phrases('Positive.tsv', to_lower=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pos_set = set()\n",
    "for k in pos_phrases_dict.keys():\n",
    "    p = \" \".join(tokenizer.findall(k))\n",
    "    if p != '':\n",
    "        pos_set.add(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pos_list = list(pos_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(vocabulary=pos_list, lowercase=True, ngram_range=(1,20), binary=True, token_pattern=token_pattern)\n",
    "X = vectorizer.fit_transform(train_corpus[y_train==1])\n",
    "vocab = vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_counts = np.sum(X, axis=0)\n",
    "all_counts_array = all_counts.A1\n",
    "all_counts_sorted_indices = np.argsort(all_counts_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(all_counts_array==0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mazing in both its irony and its technical complexity\n",
      "eal winner\n",
      "his is an aggressive unsettling glorious deeply emotional wildly imaginative piece of storytelling that you'll never forget\n"
     ]
    }
   ],
   "source": [
    "for i in all_counts_sorted_indices:\n",
    "    if all_counts_array[i] > 0:\n",
    "        break\n",
    "    terms = tokenizer.findall(vocab[i])\n",
    "    if len(terms) <= 20:\n",
    "        print(vocab[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
